---
title: "Final Exam Practice"
output: pdf_document
---


```{r, include=FALSE}
library(tidyverse)
library(mvtnorm)
knitr::opts_chunk$set(echo=TRUE, 
                      cache=FALSE, 
                      fig.width=5, 
                      fig.height=5,
                      fig.align='center',
                      echo=FALSE)
indent1 = '    '
indent2 = paste(rep(indent1, 2), collapse='')
indent3 = paste(rep(indent1, 3), collapse='')
r = function(x, digits=2){ round(x, digits=digits) }
ig <- function(file) {knitr::include_graphics(file)}

```


\subsection{Pre-midterm material}

- Likelihood
  + Identify proportionality constants that can be excluded
- Cromwell's Rule
- Sufficient Statistics
- Data Generating Process
- Bias, Variance, Mean Squared Error
- Mixture Model
- Conjugate Prior
  + Pseudo-counts interpretations of conjugate priors
- Improper Priors
- Posterior Predictive Distribution
  + Integral definition involving likelihood and posterior (or prior)


\subsection{Post-midterm material}

- Posterior Predictive Model Checking
  + Monte Carlo algorithm for checking
- Law of the unconcsious statistician (LOTUS)
- Monte Carlo error 
  + How does the variance of our Monte Carlo 
- Inversion Sampling
- Rejection Sampling
- The normal distribution
  + Basic properties of the normal distribution
  + Central limit theorem
- Bayesian inference for $\mu$ when $\sigma^2$ known
  + Conjugate prior for $\mu$ is also normal
  + Posterior distribution under the conjugate prior
  + Interpretation of the prior and posterior parameters, pseudocounts
  + Add relevant formulas to cheat sheet!
- Bias-Variance tradeoff of Bayes estimators
  + Bayes estimators add bias but reduce variance (why?).
- Decision Theory
  + Bayes estimator,s Bayes risk and loss function definitions
  + Bayes estimator for minimizing the Bayes risk under squared error loss
  + Bayes estimator for minimizing the Bayes risk under absolute error loss
-  Bayesian inference for $\mu$ and $\sigma^2$ (both unknown)
  - Sampling from the joint posterior distribution
- Markov Chains
  + Definition of a Markov Chain
  + Limiting distribution
  + Why/how they are useful in Monte Carlo sampling
- Metropolis-Hastings Algorithm
  + How to determine whether the sample should be accepted
  + Intuition of the Metropolis algorithm
  + Computational considerations
- Hastings correction (allows for non-symmetric proposals)
- Gibbs sampling
  + Basic idea of Gibbs sampling
  + Identify the full conditional distributions
  + Pros and cons of a Gibbs sampling relative to MH sampling
- MCMC convergence Diagnostics 
  + Run mulitple chains, different initalizations
  + ACF
  + Traceplot
  + Rejection rate
  + Effective sample size
  + How the size of the "jump" proposal effects the sampler


<!-- practice_2.pdf -->
<!-- Problems 2: 6, 7 -->
<!-- Problems 3: 7(a), 7(b), 8(a)(b)(c) -->
<!-- Problem 5: 3 (note: be careful with notation.  They use tau to be precision where is in our class we've used it to be a variance or sd) -->

\section{Practice Problems}


1. We observe a sample of 10 observations from a normal distribution with mean $\mu$ and precision $\frac{1}{\sigma^2}$ The data, $y_1,..., y_{10}$, are such that

    #.  Suppose the value of $\frac{1}{\sigma^2}$ is known to be 0.004 and that our prior distribution is $p(\mu) \sim N(\mu_0=20, \sigma^2=100)$.  Find $p(\mu \mid y_1, ... y_{10})$. What is the 95\% HPD interval for $\mu$?

    #.  What is the posterior mean _estimate_ for the observed data? 
    #.  Now consider the posterior mean as an _estimator_ by ignoring the observed values $y_1, ... y_{10}$ and treat $Y_1, ... Y_{10}$ as random variables.  
    #.  What is the bias of the posterior mean, $E[\mu \mid Y_1, ... Y_{10}]$?
    #. What is the variance of the posterior mean, $E[\mu \mid Y_1, ... Y_{10}]$?
    #.  What is the MSE of $E[\mu \mid Y_1, ... Y_{10}]$?
    #. How close must the true $\mu$ be to the prior $\mu_0$ for the posterior mean estimator have equal MSE to the the maximum likelihood estimator, $\bar Y$?
    
    
2.  Draw a picture of a traceplot of a Markov Chain with high / low rejection rate.
\vspace{3in}
3.  Consider Bayesian inference for p($\mu$, $\frac{1}{\sigma^2} \mid y)$, where  an $y \sim N(\mu, \sigma^2)$.  Assume the non-infromative prior disribution $p(\mu, \sigma^2) \propto 1/\sigma^2$.  This will lead to the diamond shaped posterior discussed in class and on the lecture notes.  Argue in words (no math needed) why the diamond shaped posterior makes sense? For what values of $\frac{1}{\sigma^2}$ does $\mu$ have the most posterior variability?  Lead posterior variability? Why?
\vspace{3in}

\pagebreak

4. First try this without referring to the lecture notes. Consider the following figure from the IQ example discussed in class.  This figure is based on the following model: $p(y \mid \mu, \sigma^2) \sim N(\mu, 13^2)$ and $p(\mu) \sim N(100, \frac{13^2}{\kappa_0})$.  

```{r, fig.width=6, fig.height=3, out.width="85%", warning=FALSE}

cols <- colorspace::sequential_hcl(3, "BluGrn")
  
par(mfrow = c(1, 2), mar = c(3, 3, 1, 1),  mgp = c(1.75, .75, 0))
  
b <- (100 - 112) ^ 2
s2 <- 13 ^ 2
n <- 1:50

k <- 1
brk1 <- (n / (k + n)) ^ 2 + n * (k / (k + n)) ^ 2 * b / s2
k <- 2
brk2 <- (n / (k + n)) ^ 2 + n * (k / (k + n)) ^ 2 * b / s2
k <- 3
brk3 <- (n / (k + n)) ^ 2 + n * (k / (k + n)) ^ 2 * b / s2
  
plot(range(n),
     c(0.4, 1.1),
     type = "n",
     xlab = "sample size",
     ylab = "relative MSE")
abline(h = 1, lty = 2, lwd = 2)
lines(n, brk1, col = cols[1], lwd = 2)
lines(n, brk2, col = cols[2], lwd = 2)
lines(n, brk3, col = cols[3], lwd = 2)
abline(v=10)
text(12, 1.07, "A")
text(8, 0.94, "B")
text(12, 0.88, "C")
text(1, 1.02, "D")
# legend(20,.8,
#   legend = c(
#     expression(kappa[0] == 0),
#     expression(kappa[0] == 1),
#     expression(kappa[0] == 2),
#     expression(kappa[0] == 3)
#   ),
#   lwd = c(2, 2, 2),
#   lty = c(2, 1, 1, 1),
#   col = c("black", cols),
#   bty = "n"
# )

####
theta0 <- 112
mu0 <- 100
n <- 10
s2m <- s2 / n
x <- seq(theta0 - 4 * sqrt(s2m), theta0 + 4 * sqrt(s2m), length = 100)
plot(
  x,
  dnorm(x, theta0, sqrt(s2m)),
  type = "l",
  lwd = 2,
  ylim = c(0, .13),
  lty = 2,
    xlab = "IQ",
    ylab = ""
  )
  abline(v = theta0)
  for (k in 1:3) {
    w <- n / (n + k)
    lines(
      x,
      dnorm(x, w * theta0 + (1 - w) * mu0, sqrt(w ^ 2 * s2m)),
      type = "l",
      col = cols[k],
      lwd = 2
    )
  }

```


  a.  The left figure shows the mean squared error (MSE) of the posterior mean estimator relative to the maximum likelihood estimator.  Fill in the blanks with the number 0, 1, 2, or 3. For line A $\kappa_0 =$ ____, for line B $\kappa_0 =$ ____, for line C, $\kappa_0 =$ ____, and for line D $\kappa_0 =$ ____.
    
  b.  Circle one.  The right figure depicts:
      i. The posterior distribution for $\mu$ for each value of $\kappa_0$.
      ii. The sampling distribution of the Bayes estimator, $\hat \mu$ for each value of $\kappa_0$.
      iii. The likelihood of $\mu$ for each value of $\kappa_0$.
      iv. The prior distribution of $\mu$ for each value of $\kappa_0$.
\pagebreak
  c.  Circle all options that could describe the differences between the two figures below.
  
```{r, fig.width=6, fig.height=3, out.width="85%", warning=FALSE}
par(mfrow = c(1, 2), mar = c(3, 3, 1, 1),  mgp = c(1.75, .75, 0))
###
theta0 <- 112
mu0 <- 100
n <- 10
s2m <- s2 / n
x <- seq(theta0 - 4 * sqrt(s2m), theta0 + 4 * sqrt(s2m), length = 100)
plot(
  x,
  dnorm(x, theta0, sqrt(s2m)),
  type = "l",
  lwd = 2,
  ylim = c(0, .2),
  lty = 2,
    xlab = "IQ",
    ylab = ""
  )
  abline(v = theta0)
  multiplier <- 3
  for (k in multiplier*c(1:3)) {
    w <- n / (n + k)
    lines(
      x,
      dnorm(x, w * theta0 + (1 - w) * mu0, sqrt(w ^ 2 * s2m)),
      type = "l",
      col = cols[k/multiplier],
      lwd = 2
    )
  }
  
  
  ###
theta0 <- 112
mu0 <- 100
n <- 30
s2m <- s2 / n
x <- seq(theta0 - 4 * sqrt(s2m), theta0 + 4 * sqrt(s2m), length = 100)
plot(
  x,
  dnorm(x, theta0, sqrt(s2m)),
  type = "l",
  lwd = 2,
  ylim = c(0, .2),
  lty = 2,
    xlab = "IQ",
    ylab = ""
  )
  abline(v = theta0)
  multiplier <- 1
  for (k in multiplier*c(1:3)) {
    w <- n / (n + k)
    lines(
      x,
      dnorm(x, w * theta0 + (1 - w) * mu0, sqrt(w ^ 2 * s2m)),
      type = "l",
      col = cols[k/multiplier],
      lwd = 2
    )
  }

    ```
    

  
  i. In the left figure, the values of $\kappa_0$ for each corresponding line are larger than they are for the right figure.  Both have the same sample size, $n$.
  ii. In the left figure, the values of $\kappa_0$ for each corresponding line are smaller than they are for the right figure.  Both have the same sample size, $n$.
  iii. In the left figure, $n$ is larger than it is for the right figure.  Both have the same values of $\kappa_0$.
  iv.  In the left figure, $n$ is smaller than it is for the right figure.  Both have the same values of $\kappa_0$.
  
\pagebreak

\section{Multiple Choice}

1. The Bayes estimator (estimator which minimizes the posterior expected loss) for squared error loss is:

    #.  The posterior mean
    #.  The posterior median
    #.  The posterior mode
    #.  The posterior variance

\vspace{.5cm}

2. Monte Carlo sampling is an algorithm for...
    #. reducing the bias of an estimator.
    #. approximating integrals computationally.
    #. reducing the rejection rate of the rejection sampler.
    #. minimzing the Bayes risk
    
\vspace{.5cm}

3. A sequence of random events, indexed in time, is called a _Markov Chain_ if (circle one)

    #.  the distribution of the next state depends only on the _most recent_ state
    #.  the distribution of the next state depends on the full history of states
    #.  the sequence has a limiting distribution
    #.  the sequence converges to the posterior distribution, $p(\theta \mid y)$


\vspace{.5cm}

4.  Let $y_1, ... y_n \sim N(\mu, \sigma^2)$ with $\sigma^2$ known.  You specify the conjugate prior $\mu \sim N(\mu_0, \frac{\sigma^2}{\kappa_0})$.  Assume $\kappa_0 > 0$ and that $\mu_0 \neq \mu$.  Select all answers that _must_ be true about estimators of $\mu$.  

    #.  The posterior mean estimator is biased
    #.  The maximum likelihood estimator is biased
    #.  The posterior mean has lower MSE than the MLE
    #.  The posterior mean has lower variance than the MLE
    #.  The posterior variance is less than $\frac{\sigma^2}{n}$

\vspace{.5cm}    
5. An improper prior distribution (select all that are true):

    #. can't be used for valid Bayesian inference
    #. can only be used if the posterior distribution is integrable
    #. is another name for the uniform prior distribution
    #. integrates to infinity
    
\vspace{6cm}

6. True or false: in the context of hierarchical models, if there are some true population differences between groups, then the complete pooling estimator will always be worse (in terms of mean squared error) than the no pooling estimator.


7. In the Metropolis Algorithm... (circle all that are true)
    #. The proposal distribution must be symmetric
    #. A proposed sample is always accepted if it would increase the posterior density
    #. It's best to have high autocorrelation
    #. The most efficient samplers have a rejection rate that is close to 0
    
8.  Consider esimates of mean parameter, $\theta_i$, across multiple groups of observations.  When considering the variability of the resulting estimates $\theta_i$ between groups, order the following estimates from least to most variability between groups: complete pooling, no pooling and partial pooling. estimates.

9.  Name two MCMC diagnostics that can be used to assess the quality of estimates derived from a sampler.  